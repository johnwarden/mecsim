Overall optimal point: [0.6621621621621622, 0.3378378378378378]
Overall optimal utility: 0.906764700582363

=== Round 1 ===
Current report matrix:
3×2 Matrix{Float64}:
 0.9  0.1
 0.9  0.1
 0.1  0.9
Current allocation: [0.8, 0.1]
Current user utilities: [0.980150914025541, 0.980150914025541, 0.5828743352512207]
Current overall optimalities: 0.934890150910141
Voter 1's turn.
  Best response = [0.6934663582200592, 0.17336297520821642]
  New allocation: [0.8, 0.19999666430596288]
  => Voter 1 improves by switching to best response
  Old utility = 0.980150914025541
  New utility = 0.9899493691493857
  Honest utility = 0.980150914025541
  Incentive Alignment = 0.9269412174887112
Voter 2's turn.
  => No improvement found; voter 2 stays with old report.
Voter 3's turn.
  => No improvement found; voter 3 stays with old report.

=== Round 2 ===
Current report matrix:
3×2 Matrix{Float64}:
 0.693466  0.173363
 0.9       0.1
 0.1       0.9
Current allocation: [0.8, 0.19999666430596288]
Current user utilities: [0.9899493691493857, 0.9899493691493857, 0.707103244188824]
Current overall optimalities: 0.9877615735599646
Voter 1's turn.
  => No improvement found; voter 1 stays with old report.
Voter 2's turn.
  => No improvement found; voter 2 stays with old report.
Voter 3's turn.
  => No improvement found; voter 3 stays with old report.
Converged! Maximum improvement in utility < 1.0e-5.
Final reports:
3×2 Matrix{Float64}:
 0.693466  0.173363
 0.9       0.1
 0.1       0.9
Final Allocation: [0.8, 0.19999666430596288]
Mean Utility: 0.8956673274958651
Optimality: 0.8956673274958651
Envy: 28.284612496056173
Incentive Alignment: 0.9269412174887112
