Optimal points: [0.8 0.1; 0.1 0.7]
Starting allocation: [0.45, 0.39999999999999997]

=== Round 1 ===
Current report matrix:
2×2 Matrix{Float64}:
 0.8  0.1
 0.1  0.7
Current allocation: [0.45, 0.39999999999999997]
Voter 1's turn.
  Best response = [0.999999983789506, 4.331846977014185e-8]
  New allocation: [0.549999991894753, 0.35000002165923483]
  => Voter 1 improves by switching to best response
  Old utility = 0.6730769230769231
  New utility = 0.8076922847965516
  Honest utility = 0.6730769230769231
  Incentive Alignment = 0.888196618060867
Voter 2's turn.
  Best response = [5.528159479258314e-8, 0.9999999301081106]
  New allocation: [0.50000001641113, 0.4999999835888701]
  => Voter 2 improves by switching to best response
  Old utility = 0.35000004491237224
  New utility = 0.599999960613287
  Honest utility = 0.35000004491237224
  Incentive Alignment = 0.730082776945869

=== Round 2 ===
Current report matrix:
2×2 Matrix{Float64}:
 1.0         4.33185e-8
 5.52816e-8  1.0
Current allocation: [0.50000001641113, 0.4999999835888701]
Voter 1's turn.
  => No improvement found; voter 1 stays with old report.
  Old utility = 0.6153846507316636
  New utility = 0.6153846787414875
  Honest utility = 0.4423077747138249
  Incentive Alignment = 0.730082776945869
Voter 2's turn.
  => No improvement found; voter 2 stays with old report.
  Old utility = 0.599999960613287
  New utility = 0.6000000078752932
  Honest utility = 0.35000004491237224
  Incentive Alignment = 0.730082776945869
Converged! Maximum improvement in utility < 0.0001.
Final reports:
2×2 Matrix{Float64}:
 1.0         4.33185e-8
 5.52816e-8  1.0
Final Allocation: [0.50000001641113, 0.4999999835888701]
Mean Utility: 0.6076923056724752
Optimality: 0.6076923056724752
Envy: 1.538469011837662
Incentive Alignment: 0.730082776945869
