Optimal points: [0.8 0.1; 0.1 0.5]
Starting allocation: [0.44999999999999996, 0.3]

=== Round 1 ===
Current report matrix:
2×2 Matrix{Float64}:
 0.8  0.1
 0.1  0.5
Current allocation: [0.44999999999999996, 0.3]
Voter 1's turn.
  Best response = [0.9999999751752515, 1.683429668722949e-7]
  New allocation: [0.5499999875876258, 0.2500000841714834]
  => Voter 1 improves by switching to best response
  Old utility = 0.7499999999999999
  New utility = 0.8692307208344009
  Honest utility = 0.7499999999999999
  Incentive Alignment = 0.8881966498695856
Voter 2's turn.
  Best response = [5.468577073232998e-9, 0.9999246711110352]
  New allocation: [0.4999999903219143, 0.49996241972700106]
  => Voter 2 improves by switching to best response
  Old utility = -0.01923056439664907
  New utility = 0.38461540896227536
  Honest utility = -0.01923056439664907
  Incentive Alignment = 0.6332826076456619

=== Round 2 ===
Current report matrix:
2×2 Matrix{Float64}:
 1.0         1.68343e-7
 5.46858e-9  0.999925
Current allocation: [0.4999999903219143, 0.49996241972700106]
Voter 1's turn.
  => No improvement found; voter 1 stays with old report.
  Old utility = 0.615430856921955
  New utility = 0.6154309186981647
  Honest utility = 0.44235984425977615
  Incentive Alignment = 0.6332826076456619
Voter 2's turn.
  => No improvement found; voter 2 stays with old report.
  Old utility = 0.38461540896227536
  New utility = 0.3846154165947414
  Honest utility = -0.01923056439664907
  Incentive Alignment = 0.6332826076456619
Converged! Maximum improvement in utility < 0.0001.
Final reports:
2×2 Matrix{Float64}:
 1.0         1.68343e-7
 5.46858e-9  0.999925
Final Allocation: [0.4999999903219143, 0.49996241972700106]
Mean Utility: 0.5000231329421152
Optimality: 0.5000231329421152
Envy: 23.08154479596796
Incentive Alignment: 0.6332826076456619
